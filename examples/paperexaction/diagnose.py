#!/usr/bin/env python3
"""
å¿«é€Ÿè¯Šæ–­è„šæœ¬ï¼šåˆ†æž prompt_tuner_trials.jsonlï¼ŒæŸ¥çœ‹æ˜¯å¦æ­£ç¡®è®°å½•äº†æ ·æœ¬å±•å¼€å’Œ LLM å“åº”ã€‚
"""
import json
from pathlib import Path

def diagnose_trials(log_path: str = "prompt_tuner_trials.jsonl"):
    """åˆ†æž trial æ—¥å¿—ï¼Œè¾“å‡ºè¯Šæ–­ä¿¡æ¯ã€‚"""
    log_file = Path(log_path)
    if not log_file.exists():
        print(f"âŒ {log_path} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ --train-prompt")
        return
    
    lines = log_file.read_text(encoding="utf-8").strip().split("\n")
    if not lines or not lines[0]:
        print(f"âŒ {log_path} ä¸ºç©º")
        return
    
    trials = [json.loads(line) for line in lines if line]
    print(f"ðŸ“Š åˆ†æž {len(trials)} ä¸ª trial è®°å½•\n")
    
    # æ£€æŸ¥å„ä¸ªå…³é”®å­—æ®µ
    has_sample_len = sum(1 for t in trials if "sample_expanded_prompt_len" in t)
    has_sample_response = sum(1 for t in trials if "sample_llm_response" in t)
    has_sample_expand_err = sum(1 for t in trials if "sample_expand_error" in t)
    has_sample_llm_err = sum(1 for t in trials if "sample_llm_error" in t)
    
    print(f"âœ“ åŒ…å« sample_expanded_prompt_len: {has_sample_len}/{len(trials)}")
    print(f"âœ“ åŒ…å« sample_llm_response: {has_sample_response}/{len(trials)}")
    print(f"âœ“ åŒ…å« sample_expand_error: {has_sample_expand_err}/{len(trials)}")
    print(f"âœ“ åŒ…å« sample_llm_error: {has_sample_llm_err}/{len(trials)}")
    
    # æ ·æœ¬åˆ†æž
    if trials:
        t = trials[0]
        print(f"\nðŸ” ç¬¬ä¸€ä¸ª trial çš„è¯¦ç»†ä¿¡æ¯ï¼ˆtrial #{t.get('trial')}ï¼‰:")
        print(f"  - avg_score: {t.get('avg_score')}")
        print(f"  - model: {t.get('model_params', {}).get('model')}")
        if "sample_expanded_prompt_len" in t:
            print(f"  - å±•å¼€çš„ prompt é•¿åº¦: {t['sample_expanded_prompt_len']} å­—ç¬¦")
            if t['sample_expanded_prompt_len'] > 0:
                print(f"    âœ“ {{{{content}}}} å·²æ­£ç¡®å±•å¼€")
            else:
                print(f"    âŒ {{{{content}}}} å±•å¼€å¤±è´¥æˆ–æ–‡ä»¶å†…å®¹ä¸ºç©º")
        
        if "sample_expand_error" in t:
            print(f"  - å±•å¼€é”™è¯¯: {t['sample_expand_error']}")
        
        if "sample_llm_response" in t:
            resp = t['sample_llm_response']
            print(f"  - LLM å“åº”ï¼ˆå‰ 100 å­—ç¬¦ï¼‰: {resp[:100]}")
            if resp.strip():
                try:
                    parsed = json.loads(resp)
                    print(f"    âœ“ è¿”å›žæœ‰æ•ˆçš„ JSONï¼ˆåŒ…å« {len(parsed)} æ¡è®°å½•ï¼‰")
                except json.JSONDecodeError:
                    print(f"    âŒ è¿”å›žçš„ä¸æ˜¯æœ‰æ•ˆ JSON")
            else:
                print(f"    âŒ LLM è¿”å›žç©ºå“åº”")
        
        if "sample_llm_error" in t:
            print(f"  - LLM è°ƒç”¨é”™è¯¯: {t['sample_llm_error']}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nðŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    non_zero_scores = sum(1 for t in trials if t.get('avg_score', 0) > 0)
    print(f"  - avg_score > 0 çš„ trial: {non_zero_scores}/{len(trials)}")
    
    if non_zero_scores > 0:
        best = max(trials, key=lambda t: t.get('avg_score', 0))
        print(f"\nðŸ† æœ€ä½³é…ç½®ï¼ˆscore={best['avg_score']}ï¼‰:")
        print(f"  - model: {best.get('model_params', {}).get('model')}")
        print(f"  - temperature: {best.get('model_params', {}).get('temperature')}")
    else:
        print(f"\nâš ï¸  æ²¡æœ‰ä»»ä½• trial çš„ avg_score > 0ï¼Œå¯èƒ½åŽŸå› ï¼š")
        print(f"  1. LLM è¿”å›žçš„ä¸æ˜¯æœ‰æ•ˆ JSON")
        print(f"  2. JSON ä¸­å­—æ®µä¸ç¬¦åˆé¢„æœŸï¼ˆç¼ºå°‘ material/property/valueï¼‰")
        print(f"  3. API è°ƒç”¨å¤±è´¥")
        if trials[0].get("sample_llm_error"):
            print(f"  - æŸ¥çœ‹ sample_llm_error å­—æ®µèŽ·å–è¯¦ç»†ä¿¡æ¯")

if __name__ == "__main__":
    import sys
    log_path = sys.argv[1] if len(sys.argv) > 1 else "prompt_tuner_trials.jsonl"
    diagnose_trials(log_path)

