# å‚æ•°åŒ–å’Œ Trial æ—¥å¿—å¢å¼º - å®Œæ•´è¯´æ˜

## ğŸ“‹ é—®é¢˜æ€»ç»“

### é—®é¢˜ 1ï¼šModel åç§°ç¡¬ç¼–ç 
**ç°è±¡**ï¼šä»£ç å¤šå¤„ç¡¬ç¼–ç äº† `"deepseek-r1:671b-64k"`ï¼Œç”¨æˆ·æƒ³è¦æŒ‡å®šå…¶ä»–æ¨¡å‹ï¼ˆå¦‚ `"gpt-4"` æˆ– `"claude-3""`ï¼‰ä½†æ— æ³•é€šè¿‡ CLI å‚æ•°é…ç½®ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šæ·»åŠ äº† `--model` CLI å‚æ•°ï¼Œå…è®¸åœ¨è¿è¡Œæ—¶åŠ¨æ€æŒ‡å®šæ¨¡å‹ã€‚

---

### é—®é¢˜ 2ï¼šTrial æ—¥å¿—ä¸­ avg_score å…¨ä¸º 0.0ï¼Œcontent ä¸ºç©º
**ç°è±¡**ï¼šè¿è¡Œ `--train-prompt` åï¼Œ`prompt_tuner_trials.jsonl` ä¸­æ‰€æœ‰ trial çš„ `avg_score` éƒ½æ˜¯ 0.0ï¼Œçœ‹ä¸åˆ° LLM çš„å®é™…è¿”å›å†…å®¹ã€‚

**æ ¹æœ¬åŸå› **ï¼š
1. æ—¥å¿—ä¸­åªè®°å½•äº† prompt **template**ï¼ˆå« `{{content}}` å ä½ç¬¦ï¼‰ï¼Œè€Œéå±•å¼€åçš„å®é™… prompt
2. æ²¡æœ‰è®°å½• LLM çš„å®é™…å“åº”ï¼Œæ— æ³•çœ‹åˆ°æ˜¯å¦è¿”å›äº†æœ‰æ•ˆ JSON
3. æ²¡æœ‰è®°å½•é”™è¯¯ä¿¡æ¯ï¼Œæ— æ³•è¯Šæ–­åˆ†æ•°ä¸º 0 çš„åŸå› 

**è§£å†³æ–¹æ¡ˆ**ï¼š
- åœ¨ trial æ—¥å¿—ä¸­æ·»åŠ  `sample_expanded_prompt_len`ï¼šç¬¬ä¸€ä¸ªæ ·æœ¬å±•å¼€å prompt çš„å­—ç¬¦æ•°ï¼ˆéªŒè¯ `{{content}}` æ˜¯å¦è¢«å±•å¼€ï¼‰
- æ·»åŠ  `sample_llm_response`ï¼šLLM å®é™…è¿”å›çš„å‰ 200 ä¸ªå­—ç¬¦ï¼ˆå¯æŸ¥çœ‹æ˜¯å¦æ˜¯æœ‰æ•ˆ JSONï¼‰
- æ·»åŠ  `sample_expand_error` å’Œ `sample_llm_error`ï¼šä»»ä½•å¼‚å¸¸ä¿¡æ¯

---

## ğŸ”§ ä»£ç ä¿®æ”¹è¯¦æƒ…

### 1. paper_extraction.py

**æ·»åŠ  `--model` å‚æ•°**ï¼š
```python
p.add_argument("--model", default="deepseek-r1:671b-64k", help="LLM model name")
```

**ä¼ é€’ç»™ train_prompt()**ï¼š
```python
best = train_prompt(args.dev_labeled, tp_api_url, tp_api_key, model=args.model)
```

**ä¼ é€’ç»™ extract_with_llm()**ï¼š
```python
results = extract_with_llm(expanded_files, prompt_cfg, api_url, api_key, model=args.model)
```

**æ·»åŠ  fail-fast æ£€æŸ¥**ï¼š
```python
if args.use_llm and not tp_api_url:
    raise SystemExit(
        "LLM API URL not provided for --train-prompt; set PAPER_LLM_API_URL env var or pass --api-url <URL> --api-key <KEY>"
    )
```

### 2. prompt_tuner.py

**æ·»åŠ  model å‚æ•°åˆ° train_prompt() ç­¾å**ï¼š
```python
def train_prompt(
    dev_path: str,
    api_url: str | None = None,
    api_key: str | None = None,
    model: str = "deepseek-r1:671b-64k",
    candidates: List[str] | None = None,
    trials: int = 200,
) -> Dict[str, Any]:
```

**åœ¨å¾ªç¯ä¸­ä½¿ç”¨ model å‚æ•°ç”Ÿæˆ model_params**ï¼š
```python
model_params = {"model": model, "temperature": random.choice([0.0, 0.2, 0.5])}
```

**åœ¨è¯•éªŒæ—¥å¿—ä¸­è®°å½•è¯¦ç»†ä¿¡æ¯**ï¼š
```python
rec = {
    "trial": i,
    "prompt_template": prompt_template,
    "model_params": model_params,
    "avg_score": avg,
}

# è®°å½•æ ·æœ¬å±•å¼€çš„ prompt é•¿åº¦å’Œ LLM å“åº”
for item in dev[:1]:  # è®°å½•ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ä¿¡æ¯ç”¨äºè¯Šæ–­
    f = item["file"]
    try:
        expanded_prompt = prompt_template.replace("{{content}}", Path(f).read_text(encoding="utf-8")).replace("{{filename}}", Path(f).name)
        rec["sample_expanded_prompt_len"] = len(expanded_prompt)
        if api_url:
            try:
                resp = call_llm(expanded_prompt, api_url, api_key, model_params)
                rec["sample_llm_response"] = resp[:200] if isinstance(resp, str) else str(resp)[:200]
            except Exception as e:
                rec["sample_llm_error"] = str(e)[:100]
    except Exception as e:
        rec["sample_expand_error"] = str(e)[:100]

with trials_log.open("a", encoding="utf-8") as fh:
    fh.write(json.dumps(rec, ensure_ascii=False) + "\n")
```

**å¯¼å…¥ call_llm**ï¼š
```python
from llm_client import call_llm
```

### 3. llm_client.py

**æ— éœ€ä¿®æ”¹**ï¼šå·²ç»æ”¯æŒä» `model_params` ä¸­æå– `model` å­—æ®µ
```python
model = (model_params or {}).get("model", "deepseek-r1:671b-64k")
```

---

## ğŸ“– ä½¿ç”¨æ–¹æ³•

### åŸºç¡€ä½¿ç”¨ï¼ˆé»˜è®¤æ¨¡å‹ï¼‰
```bash
cd examples/paperexaction

# ä»…ä½¿ç”¨è§„åˆ™æŠ½å–
python paper_extraction.py \
  --template template.json \
  samples/*.md

# ç»“æœä¼šä¿å­˜åˆ° results.csvï¼ˆé»˜è®¤ï¼‰
```

### ä½¿ç”¨ LLMï¼ŒæŒ‡å®šæ¨¡å‹
```bash
# ç¯å¢ƒå˜é‡æ–¹å¼
export PAPER_LLM_API_URL="https://uni-api.cstcloud.cn/v1"
export PAPER_LLM_API_KEY="your-api-key"

# CLI è°ƒç”¨
python paper_extraction.py \
  --template template.json \
  --use-llm \
  --model "gpt-4" \
  samples/*.md
```

### è¿è¡Œ Prompt è°ƒä¼˜ï¼ˆæŒ‡å®šæ¨¡å‹ï¼‰
```bash
python paper_extraction.py \
  --template template.json \
  --use-llm \
  --train-prompt \
  --dev-labeled samples/labeled.jsonl \
  --model "deepseek-chat" \
  --api-url "https://uni-api.cstcloud.cn/v1" \
  --api-key "your-api-key" \
  samples/*.md

# ä¼šç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š
# - prompt_tuner_trials.jsonl: æ¯ä¸ª trial çš„è¯¦ç»†è®°å½•
# - prompt_tuner_run.json: æ•´ä¸ªè®­ç»ƒè¿è¡Œçš„å…ƒä¿¡æ¯
# - prompt_config.json: æœ€ä½³é…ç½®
```

---

## ğŸ” è¯Šæ–­å’Œæ’æŸ¥

### æŸ¥çœ‹ Trial æ—¥å¿—ä¸­çš„æ ·æœ¬ä¿¡æ¯

**ä½¿ç”¨è¯Šæ–­è„šæœ¬**ï¼š
```bash
python diagnose.py prompt_tuner_trials.jsonl
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
ğŸ“Š åˆ†æ 200 ä¸ª trial è®°å½•

âœ“ åŒ…å« sample_expanded_prompt_len: 200/200
âœ“ åŒ…å« sample_llm_response: 200/200
âœ“ åŒ…å« sample_expand_error: 0/200
âœ“ åŒ…å« sample_llm_error: 0/200

ğŸ” ç¬¬ä¸€ä¸ª trial çš„è¯¦ç»†ä¿¡æ¯ï¼ˆtrial #0ï¼‰:
  - avg_score: 0.05
  - model: deepseek-chat
  - å±•å¼€çš„ prompt é•¿åº¦: 1234 å­—ç¬¦
    âœ“ {{content}} å·²æ­£ç¡®å±•å¼€
  - LLM å“åº”ï¼ˆå‰ 100 å­—ç¬¦ï¼‰: [{"material": "Aluminum", "property": "density", "value": "2.78"}]
    âœ“ è¿”å›æœ‰æ•ˆçš„ JSONï¼ˆåŒ…å« 1 æ¡è®°å½•ï¼‰

ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:
  - avg_score > 0 çš„ trial: 45/200

ğŸ† æœ€ä½³é…ç½®ï¼ˆscore=0.35ï¼‰:
  - model: deepseek-chat
  - temperature: 0.0
```

### æ‰‹åŠ¨æŸ¥çœ‹ Trial æ—¥å¿—

**æŸ¥çœ‹æ ·æœ¬å±•å¼€çš„é•¿åº¦**ï¼ˆéªŒè¯ `{{content}}` æ˜¯å¦è¢«å±•å¼€ï¼‰ï¼š
```bash
cat prompt_tuner_trials.jsonl | python -c "
import json, sys
for line in sys.stdin:
    t = json.loads(line)
    if 'sample_expanded_prompt_len' in t:
        print(f\"trial {t['trial']}: expanded_len={t['sample_expanded_prompt_len']}\")
        break
"
```

**æŸ¥çœ‹ LLM å“åº”**ï¼ˆæŸ¥çœ‹è¿”å›çš„å†…å®¹ï¼‰ï¼š
```bash
cat prompt_tuner_trials.jsonl | python -c "
import json, sys
for line in sys.stdin:
    t = json.loads(line)
    if 'sample_llm_response' in t:
        print(f\"trial {t['trial']}: response={t['sample_llm_response'][:100]}\")
        break
"
```

**æŸ¥çœ‹é”™è¯¯ä¿¡æ¯**ï¼ˆè¯Šæ–­å¤±è´¥åŸå› ï¼‰ï¼š
```bash
cat prompt_tuner_trials.jsonl | python -c "
import json, sys
for line in sys.stdin:
    t = json.loads(line)
    if 'sample_llm_error' in t:
        print(f\"trial {t['trial']}: error={t['sample_llm_error']}\")
"
```

---

## ğŸ› å¸¸è§é—®é¢˜æ’æŸ¥

### æƒ…å†µ 1ï¼šsample_expanded_prompt_len = 0

**é—®é¢˜**ï¼š`{{content}}` å ä½ç¬¦æ²¡æœ‰è¢«å±•å¼€

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼š`ls samples/sample1.md`
2. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è¯»ï¼š`cat samples/sample1.md | head -5`
3. æ£€æŸ¥ prompt template ä¸­æ˜¯å¦çœŸçš„æœ‰ `{{content}}`ï¼š`grep '{{content}}' prompt_config.json`

**è§£å†³**ï¼šç¡®ä¿æ–‡ä»¶å­˜åœ¨ä¸” prompt template ä¸­åŒ…å«å ä½ç¬¦

### æƒ…å†µ 2ï¼šsample_llm_response ä¸ºç©ºï¼Œä½† sample_llm_error æœ‰å†…å®¹

**é—®é¢˜**ï¼šLLM API è°ƒç”¨å¤±è´¥

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æŸ¥çœ‹é”™è¯¯ä¿¡æ¯ï¼š`cat prompt_tuner_trials.jsonl | grep sample_llm_error`
2. æ£€æŸ¥ API æ˜¯å¦å¯è®¿é—®ï¼š
   ```bash
   curl -X POST https://uni-api.cstcloud.cn/v1/chat/completions \
     -H "Authorization: Bearer $PAPER_LLM_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{"model":"gpt-4","messages":[{"role":"user","content":"hi"}]}'
   ```
3. æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼š`echo $PAPER_LLM_API_URL $PAPER_LLM_API_KEY`

**è§£å†³**ï¼šç¡®ä¿ API URL/Key æ­£ç¡®ä¸”ç½‘ç»œå¯è®¿é—®

### æƒ…å†µ 3ï¼šsample_llm_response æœ‰å†…å®¹ä½† avg_score = 0.0

**é—®é¢˜**ï¼šLLM è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆ JSONï¼Œæˆ– JSON æ ¼å¼ä¸æœŸæœ›ä¸ç¬¦

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æŸ¥çœ‹å®é™…è¿”å›ï¼š`cat prompt_tuner_trials.jsonl | python -m json.tool | grep -A 2 sample_llm_response | head -20`
2. å°è¯•è§£æï¼š
   ```bash
   cat prompt_tuner_trials.jsonl | python -c "
   import json, sys
   for line in sys.stdin:
       t = json.loads(line)
       if 'sample_llm_response' in t:
           try:
               parsed = json.loads(t['sample_llm_response'])
               print(f'âœ“ æœ‰æ•ˆ JSON: {type(parsed).__name__}')
           except json.JSONDecodeError as e:
               print(f'âœ— æ— æ•ˆ JSON: {e}')
   "
   ```

**è§£å†³**ï¼šè°ƒæ•´ prompt template æˆ– JSON è§£æé€»è¾‘ä»¥åŒ¹é… LLM çš„è¾“å‡ºæ ¼å¼

---

## ğŸ“Š æ—¥å¿—æ–‡ä»¶æ ¼å¼

### prompt_tuner_trials.jsonl

æ¯è¡Œæ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼Œè¡¨ç¤ºä¸€ä¸ª trialï¼š

```json
{
  "trial": 0,
  "prompt_template": "Extract material properties...",
  "model_params": {
    "model": "deepseek-chat",
    "temperature": 0.0
  },
  "avg_score": 0.05,
  "sample_expanded_prompt_len": 1234,
  "sample_llm_response": "[{\"material\":\"Al\",\"property\":\"density\",\"value\":\"2.78\"}]",
  "sample_llm_error": null
}
```

å…³é”®å­—æ®µè¯´æ˜ï¼š
- `trial`ï¼štrial ç¼–å·ï¼ˆ0 å¼€å§‹ï¼‰
- `prompt_template`ï¼šä½¿ç”¨çš„ prompt æ¨¡æ¿ï¼ˆå«å ä½ç¬¦ï¼‰
- `model_params`ï¼šæ¨¡å‹å‚æ•°ï¼ˆmodelã€temperature ç­‰ï¼‰
- `avg_score`ï¼šåœ¨å¼€å‘é›†ä¸Šçš„å¹³å‡ F1 åˆ†æ•°
- `sample_expanded_prompt_len`ï¼šç¬¬ä¸€ä¸ªæ ·æœ¬å±•å¼€åçš„ prompt é•¿åº¦ï¼ˆè¯Šæ–­ç”¨ï¼‰
- `sample_llm_response`ï¼šç¬¬ä¸€ä¸ªæ ·æœ¬çš„ LLM è¿”å›ï¼ˆå‰ 200 å­—ç¬¦ï¼Œè¯Šæ–­ç”¨ï¼‰
- `sample_expand_error`ï¼šå±•å¼€ prompt æ—¶çš„é”™è¯¯ï¼ˆè¯Šæ–­ç”¨ï¼‰
- `sample_llm_error`ï¼šè°ƒç”¨ LLM æ—¶çš„é”™è¯¯ï¼ˆè¯Šæ–­ç”¨ï¼‰

---

## âœ… éªŒè¯æ¸…å•

åœ¨è¿è¡Œç”Ÿäº§çº§åˆ«çš„ extraction ä¹‹å‰ï¼Œæ£€æŸ¥ä»¥ä¸‹å†…å®¹ï¼š

- [ ] `--model` å‚æ•°æ˜¯å¦æ­£ç¡®è¯†åˆ«ï¼Ÿè¯•è¿è¡Œ `python paper_extraction.py --help | grep model`
- [ ] Trial æ—¥å¿—ä¸­æ˜¯å¦åŒ…å« `sample_expanded_prompt_len` > 0ï¼Ÿè¿è¡Œ `python diagnose.py`
- [ ] Trial æ—¥å¿—ä¸­æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„ `sample_llm_response`ï¼Ÿæ‰‹åŠ¨æŸ¥çœ‹ç¬¬ä¸€æ¡è®°å½•
- [ ] `avg_score` æ˜¯å¦ > 0ï¼Ÿå¦‚æœä¸æ˜¯ï¼Œè¯Šæ–­ sample_llm_response æˆ– sample_llm_error
- [ ] `prompt_config.json` æ˜¯å¦è¢«æ­£ç¡®ç”Ÿæˆï¼Ÿæ£€æŸ¥æ–‡ä»¶ `ls -la prompt_config.json`

